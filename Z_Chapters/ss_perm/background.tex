\section{研究背景}
随着机器学习在各个领域的广泛应用，数据隐私问题日益严重。
%
训练机器学习模型所需的数据可能分散在多个参与方中，比如两家医院分别有患者的CT图像数据和患者的心电图数据；银行和电商平台分别有用户的金融数据和消费数据。
%
直接把数据汇聚到一个服务器上进行中心化的模型训练，会直接暴露参与方的数据隐私。
%
类似地，在机器学习模型的部署应用阶段，用户需要将数据发送给服务器来获得模型的预测结果，这也暴露了用户的隐私数据。

%
为了解决隐私问题，许多隐私保护机器学习的方案被提出。
%
比如（横向）联邦学习（Federated Learning）~\cite{yangqiang2019federated,mcmahan_2017_fedavg}通过各方训练本地模型再聚合的方式，保护各方本地数据不出域。
但是其仅支持数据横向分割（各个参与方有不同的训练样本，但是样本的特征列是相同的）的情况，且在联邦学习过程中，模型的参数信息也会被暴露给各个参与方。
%
此外，联邦学习主要针对模型训练场景，无法在模型推断阶段提供隐私保护。
%
对于更加复杂的数据纵向分割（各个参与方有同样的样本，但是特征列不同）的情况，或是用户使用模型对自身的输入进行推断的情形，传统的横向联邦学习就无法被应用。
%
拆分学习技术~\cite{vepakomma2018split}可以适用于数据纵向分割或是模型推断的场景。
但是由于在拆分学习的训练和推断过程中，各方需要交换中间结果，因此存在较大的隐私泄漏风险，带来了诸多安全性质疑~\cite{abuadbba2020can_split,hezecheng_2019_model_inversion_attack}。
尽管第\ref{chap:randomized_topk}、\ref{chap:peloss}章提出的方案一定程度上缓解拆分学习隐私泄露的问题，但是拆分学习交换中间结果的特性注定其无法做到对数据或模型隐私的完全保护，因此在某些安全性要求较高的领域难以应用。

为了实现计算过程中完全的、可证明的隐私保护，必须采用密码学的安全计算技术。
%
近年来，许多基于密码学的隐私保护机器学习框架被提出~\cite{mohassel2017secureml,wagh2019securenn,mohassel2018aby3}。
%
这些框架一般假设有2方或3方参与隐私保护的机器学习计算，采用秘密分享、同态加密、混淆电路等密码学基础技术，来实现安全的神经网络的推断和训练。
%
尽管基于密码学的隐私保护机器学习框架拥有可证明的安全性，其也存在着严重的效率问题。
相比于中心化的明文计算，密码学方法的通信和计算开销往往高出几个数量级。
%
由于使用密码学方法进行算术运算（加法、乘法）的技术已经相对成熟，密码学方法的性能瓶颈往往在于神经网络中的激活函数。
%
神经网络中的激活函数是非线性的，其无法用算术运算简单表示，因此需要使用开销相对较高的混淆电路协议~\cite{yao1986gc}、GMW协议~\cite{gmw_1987}或是其他的定制化协议进行安全计算，并以分段线性~\cite{mohassel2017secureml}或多项式~\cite{gilad2016cryptonets}等方法对非线性函数进行拟合。
%
另一方面，近年来也有一些研究通过将密码学与其他方法结合，设计更加高效的隐私保护机器学习框架~\cite{zhangqiao_2018_gelu_net,xiepeichen_2019_bayhenn,zhou_2022_codesign}。
%
但是这些研究一般采用了类似拆分学习的思想，部分暴露了中间结果的信息，因此存在一定程度的数据或模型的隐私泄露~\cite{wong_2020_lwe_model,abuadbba2020can_split,hezecheng_2019_model_inversion_attack}。
%
因此，如何更好地将密码学方法与非密码学方法结合以提高效率，同时尽可能地保持安全性，成为了隐私保护机器学习中亟待解决的问题。
