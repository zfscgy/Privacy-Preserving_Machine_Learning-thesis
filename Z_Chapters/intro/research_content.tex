综上所述，当前隐私保护机器学习的关键是效率和安全。
面向效率和安全问题，我们可以对当前的隐私保护机器学习领域总结出如下几条挑战：
\begin{itemize}
    \item \textbf{拆分学习的效率的进一步优化}：
    拆分学习是纵向联邦学习的一种范式，其通过将模型切分划分给各个参与方的形式来实现一定程度的隐私保护。
    %
    在拆分学习模型的推断和训练中，中间结果需要频繁交换。
    在拆分层尺寸大的情况下，会造成较大的通信开销。
    %
    不同于联邦学习中通信压缩的广泛使用，拆分学习通信效率优化的研究依然处于起步阶段。
    %
    因此，如何对拆分学习的通信效率进行优化，是拆分学习走向更广泛应用的一大挑战。

    \item \textbf{增强拆分学习的安全性}：
    %
    在拆分学习模型的推断和训练中，中间表征和部分模型暴露给了参与方，因此也带来了隐私泄漏的风险。
    已有许多研究调查了针对拆分学习中间表征的攻击，指出攻击者可以从拆分层的表征推出输入特征信息和输出标签信息，甚至恢复出完整模型。
    %
    这些攻击对拆分学习的安全性产生了严重损害。
    %
    因此，如何加强拆分学习的安全性是拆分学习应用中亟待解决的问题。

    \item \textbf{更好地混合密码学和非密码学方法来实现效率与安全的平衡}：
    虽然基于密码学算法的隐私保护机器学习系统有着可证明的安全性，但是其带来的巨大的计算和通信量使其在许多人工智能模型上难以实用。
    %
    当前的将密码学方法和拆分学习等非密码学方法结合的研究往往对安全性做出了较大的妥协，有一些研究甚至对其安全性提出了严重的质疑。
    %
    因此，探究如何更好地将密码学方法和非密码学方法结合以提高效率，同时尽可能保证其安全性影响最小，是隐私保护机器学习系统走向实用所面临的一大挑战。

    \item \textbf{将隐私保护机器学习应用到更大规模的模型中}：
    随着人工智能和深度学习的发展，模型的规模日益加大，尤其是ChatGPT等大语言模型的兴起，许多模型的参数量已经增长到数十亿甚至百亿、千亿量级。
    %
    在如此大规模的模型上采用密码学方法会带来巨大的计算和通信开销，难以达到实用水平；而由于大规模模型的中间结果包含了较多信息，导致基于拆分学习的方法的安全性风险严重增加。
    %
    因此，大规模模型的出现和流行给隐私保护机器学习带来了新的挑战。
    %
\end{itemize}


本文针对上述的四大挑战，围绕着拆分学习以及密码学和其他方法混合的隐私保护机器学习进行研究，提出了一系列创新的解决方案，对隐私保护机器学习的效率和安全进行了更好的优化和平衡，并且提出了在大语言模型上可以实用的、高安全性的隐私保护机器学习系统。
%
本文的研究内容具体如下：
%
\begin{enumerate}[label={(\arabic*)}]
    \item \textbf{基于随机Top-$k$算法的拆分学习的通信效率优化}：
    拆分学习要求每一轮训练或推断都传输拆分层的隐层表征，因此具有一定的通信开销。
    %
    在联邦学习中，稀疏和量化等通信压缩方法已经被广泛使用，但是其在拆分学习中的应用依然鲜有研究。
    %
    本文考虑标签数量大且标签信息不敏感的拆分学习场景，对缩小拆分层维度、Top-$k$稀疏化、拆分层L1正则化、拆分层量化等集中基础的通信压缩方法进行了研究。
    %
    本文通过对收敛性和泛化性的理论分析，指出了Top-$k$稀疏化理论上拥有更好的泛化性，且加入随机因素后可以解决Top-$k$面临的收敛性问题。
    %
    最终本文提出随机Top-$k$稀疏化，在保证模型准确率的基础上大幅降低了拆分学习的通信开销。
    
    \item \textbf{基于势能损失的拆分学习隐私保护}：
    拆分学习过程中暴露的中间结果带来的隐私泄露问题已经被大量研究提出，如何对此进行防护是一个亟待解决的问题。
    %
    我们针对拆分学习的拆分层表征与模型预测值相关性高的问题，以及随之而来的模型补全攻击进行理论分析，将攻击问题转化为一个有监督或无监督的机器学习问题。
    %
    通过对泛化误差和聚类误差进行分析，并受到静电平衡的物理现象启发，我们提出了基于势能损失的拆分层扰动方法。
    %
    该方法可以让各个类别的拆分层表征远离类别中心，趋向于类别的决策边界分布，降低了拆分层表征与标签的关联度，使得攻击者难以从中学习出表征到标签的映射，对拆分层的标签信息提供了有力防护。

    \item \textbf{基于秘密分享和随机排列的隐私保护神经网络}：
    基于密码学的隐私保护机器学习框架的开销主要集中在非线性激活函数的计算上，而对于矩阵乘法之类的线性运算则有较成熟的解决方案。
    %
    而现有的基于密码学和其他方法的混合框架，往往需要暴露中间层结果，在安全性上面临较大的隐患。
    %
    为此，本文利用神经网络中非线性函数的逐元素性质，基于半可信的辅助第三方，提出了基于随机排列的算法来安全地计算非线性函数，并与秘密分享的线性计算融合，实现了高效的三方隐私保护神经网络。
    %
    本文还对随机排列的安全性进行了详细的理论和实验分析，表明随机排列带来的隐私泄露可以达到忽略不计的水平，相比于拆分学习等暴露中间结果的方案显著提高了安全性。
 
    \item \textbf{高效的大语言模型安全推理}：
    大语言模型即使在明文计算情况下也需要极高的计算资源，因此目前的基于密码学的隐私保护机器学习系统应用在大模型上往往会产生更为巨大的通信和计算开销。
    %
    本文基于此前研究的基于秘密分享和随机排列的隐私保护神经网络框架，结合同态加密技术，针对大模型推理进行了进一步优化。
    %
    考虑到推理过程的权重是恒定的，本文将秘密分享的乘法计算过程分为三个阶段，并且减少了重复的秘密分享环节。
    同时，本文采用了安全的排列协议来计算随机排列，将第三方的所有工作转移到离线阶段进行，进一步提高在线阶段的效率。
    %
    本文提出的大模型隐私推断框架，实现了在常规网络环境下的数秒内两方大模型在线推理，相比于已有算法产生了数百倍的提升。
    %
\end{enumerate}