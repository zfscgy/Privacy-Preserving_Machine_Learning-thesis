\chapter{基于随机排列的高效大语言模型推理框架}
大语言模型（Large Language Models，简称大模型）的出现给隐私保护机器学习带来了新的挑战。
%
一方面，由于大模型的参数量和计算量庞大，基于密码学的隐私推断方法面临着极为严重的性能问题；另一方面，由于大模型的结构影响，拆分学习等暴露中间结果的方法无法保证隐私。
%
为了实现更为高效和安全的大模型隐私推断，本章在第\ref{chap:ss-perm}章的基础上，优化秘密分享的乘法协议和基于随机排列的非线性函数协议，同时采用同态加密实现预测层，实现了高效的大模型隐私推断框架PermLLM。
%
PermLLM框架能够在现实网络环境下实现秒级别的大模型隐私推断，比已有的基于密码学的方案提升了超过两个数量级。

\input{Z_Chapters/perm_llm/background.tex}
\input{Z_Chapters/perm_llm/problem.tex}
\input{Z_Chapters/perm_llm/method.tex}
\input{Z_Chapters/perm_llm/security_analysis.tex}
\input{Z_Chapters/perm_llm/exp.tex}
\input{Z_Chapters/perm_llm/conclusion.tex}