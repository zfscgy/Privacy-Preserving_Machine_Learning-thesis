\section{安全性分析}
本节我们对PermLLM进行的安全性进行分析，表明其泄漏的隐私是忽略不计的。
%
我们分别对随机排列和浮点数秘密分享的安全性进行分析。


\subsection{随机排列}
在PermLLM中，随机排列用于注意力分数的Softmax计算、激活函数的计算，以及最终的单词分数解码中。
%
首先考虑注意力分数，假设有$h$个注意力头，键值向量个数为$n$，则总共有$h!(n!)^h$总随机排列的方式。
即使$n=1$，也有$h!$种。
在大语言模型中，$h \ge 32$，因此$h! > 2^{117}$，因此猜测出实际排列的概率可以忽略不计。
%
而对于激活函数，则排列元素个数为隐层大小，超过$1000$；对于单词分数，则其元素个数为词汇表大小，超过$10$万。
因此这些情况恢复出原始排列的可能性更加接近0。
%
除此之外，我们在第\ref{chap:ss-perm}章中对随机排列的值和原始值的相关性进行了详尽分析，表明随机排列后的值和原始值在距离相关性~\cite{szekely2007dcor}的意义下相关程度非常低，也就是说其暴露的原始值的隐私信息也是极小的。


\textbf{暴力搜索攻击}：
一种针对随机排列的攻击是暴力搜索（Brute-Force Search）攻击。
对于随机排列后的结果$\bvec y' = \pi[f(\bvec x)]$，攻击者可以选取所有可能的输入$\bvec x' \in X$，并检查$f(\bvec x')$是否与$f(\bvec x)$的元素相同。
这是因为随机排列只会改变向量中元素的顺序，但是还是会保留原始的元素集合。
在不考虑计算资源的情况下，若函数的输入$\bvec x$的取值是有限的，则攻击者理论上可以通过暴力搜索攻击从随机排列的输出推出（可能的）原始的输入。
%
由于大语言模型输入是离散的，因此也存在暴力搜索攻击的风险。
但是PermLLM巧妙地回避了此种攻击。
%
在PermLLM中，只有$P_1$（用户）才能获得随机排列后的明文，而$P_1$本身也是离散输入的拥有方，因此不存在暴力破解的可能性。

\subsection{浮点数秘密分享}
在浮点数秘密分享中，任何一个数$x$的分享值满足
\begin{equation}
\begin{cases}
    \langle x \rangle_0 = D & \text{其中} \mathbb E[D^2] = \lambda^2 \mathbb E[x^2] \\
    \langle x \rangle_1 = x - D
\end{cases}
\end{equation}
此处为了方便，我们假设$\mathbb Ex = \mathbb ED = 0$。
%
可见$\langle x \rangle_0$与$x$无关，而$\langle x \rangle_1$是$x$加入一个规模是其$\lambda$倍的巨大噪声的结果。
%
同时我们也注意到，在执行秘密分享的乘法时，$x - u$会暴露给双方，此时$\mathbb E[u^2] = 2\lambda^2 \mathbb E[x^2]$。
%
对于秘密分享乘法的结果$z = xy$，在产生Beaver三元组的时候我们已经选取了$\mathbb E \langle W \rangle_0 = \lambda^2 \mathbb E[z^2]$，因此$\langle z \rangle_0$ 与 $z$无关，而$\langle z \rangle_0$ 是$z$加上一个规模为其$\lambda$倍的噪声。
%
综上所述可以得出，对于任何一个值$x$，采用浮点数秘密分享时，各方获取到的数据$x'$只有两种情况：
\begin{enumerate}
    \item $x'$与$x$无关；
    \item $x' = x + D$，$D$为与$x$无关的随机变量且$\mathbb E[D^2] = \lambda^2 \mathbb E[x^2]$。
\end{enumerate}
%
当$\lambda$很大时，$x'$暴露的关于$x$的信息暴露可以忽略不计。
%
但是如果使用同样的输入变量进行多次浮点秘密分享计算，则攻击者可以通过多次平均的方式来获取原始的值。
%
假设$x$是原始值，攻击者获取的加噪值为$x_1, \cdots, x_n$，则有：
\begin{equation}
\begin{split}
    \mathbb E\left[\left(\dfrac{x_1 + \cdots + x_n}{n} - x \right)^2 \right] = 
    \mathbb E\left[\dfrac{(x_1 - x)^2 + \cdots + (x_n - x)^2}{n^2} \right] =
    \dfrac{\lambda^2}{n}.
\end{split}
\end{equation}
%
若选取$\lambda = 100$，则攻击者（如$P_1$）采用同样的输入进行一万次隐私推断，并对上述秘密分享值进行平均，可以将误差缩小到原始数据规模的$1/10$水平：
$\sqrt{\mathbb E[(\bar x - x)^2]} = {\mathbb E[x^2]}/{10}$。
%
此时$P_1$可能根据这些估计的秘密分享值，也就是隐私推断中的中间结果，反推出模型权重，从而实现窃取模型的目的。
%
因此，采用浮点数秘密分享时，必须要采取额外的手段来防止上述攻击的产生。可能的方案有：采用第三方对$P_1$的输入文本进行验证；$P_0$设计一定的手段检测恶意行为；以及当隐私推断达到一定次数后，对模型进行恒等变换来改变权重值~\cite{xuhengyuan_2024_permutation_transformer}，再重新执行初始化阶段分享权重等，
%


