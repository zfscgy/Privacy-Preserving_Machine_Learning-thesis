\section{研究背景}
拆分学习是一种高效的隐私保护机器学习手段~\cite{vepakomma2018split,poirot2019split}。
在拆分学习中，模型被分割为数个部分分发给各个参与方，各个参与方在模型的训练或推断过程中只需要交换中间结果而非原始数据，在一定程度上保护了数据的和模型参数的隐私。
%
相比于基于密码学的隐私保护机器学习方法，拆分学习具有较高的通信和计算效率。
%
如今拆分学习已经被应用在机器学习的多个领域中~\cite{palanisamy2021spliteasy,fagbohungbe2022split_edge_image,ccc2022vfgnn}。

由于第\ref{chap:randomized_topk}章中已经对拆分学习进行了基本的介绍，本节对此不再赘述。
%
虽然拆分学习拥有实现简单、效率高等诸多优势，但是在训练和推断过程中，各方直接交换了中间结果和中间梯度的明文，从而存在一定的隐私泄漏风险。
%
本章主要考虑的是拆分学习中的模型和标签数据的隐私问题。
%
不失一般性地，我们考虑两方拆分学习的场景，其中用户拥有输入特征（$X$），而模型拥有方拥有模型参数并把底部模型（$M_b$）下发给用户。
一个两方拆分学习推断的流程可以表示为：
\begin{equation}
    X \stackrel{M_b}{\to} H \stackrel{M_t}{\to} Y,
\end{equation}
其中，$X$是输入特征，$H$是拆分层的隐层表征，$Y$是预测的样本标签，而$M_b, M_t$表示底部模型和顶部模型。
%
可以看出，拆分层表征$H$同输入特征$X$、预测值$Y$都具有一定的相关性，因此有一定的隐私泄露风险。


目前已经有一些研究探讨了从拆分层表征逆推输入特征的攻击（$H\to X$），并提出了对应的防御手段。
Vepakomma等人~\cite{vepakomma2020nopeek}提出了在训练过程中加入额外的基于距离相关性~\cite{szekely2007dcor}的损失函数$\text{Dcor}(H, X)$来减少表征中包含的输入信息。
%
拆分层表征泄露标签隐私（$H \to Y$）的问题更为严重。
%
Fu等人~\cite{fucong2022label_infer_attack}提出通过少量泄漏的带标签样本即可从随机初始化训练出顶部模型，从而窃取样本的标签以及整体模型，并将这种攻击称之为模型补全攻击。
%
针对这种攻击，Sun等人~\cite{sunjiankai2022forward_embedding_protect}提出使用距离相关性损失对表征和输出标签进行解耦。
%
除此之外，拆分学习训练过程中的梯度也会泄露标签隐私。
为防御训练过程中的隐私泄露问题，Li等人~\cite{oscarli2022label_defense_marvell}表明拆分层的梯度和样本标签的相关性很高，并提出了一些对于二分类模型的防御方法。
Sanjay等人~\cite{sanjay2023exploit_split_learning}在训练时采用替代的顶部模型对梯度进行匹配从而实现窃取标签的目的。
%
本文针对分类模型的拆分学习推断阶段，旨在对$H\to Y$这一隐私泄漏链条进行保护，防止模型补全攻击。

直觉而言，拆分层表征泄露模型输出信息似乎是不可避免的。
%
因为在模型训练过程中，其逐渐丢弃隐层表征中关于输入特征的信息，只保留与标签相关的信息。
但是为了让模型产生准确的预测，隐层表征中必须包含关于标签的信息。
%
因此，本文并不尝试将标签相关信息从拆分层表征中彻底删除，而是把攻击问题转化成一个监督学习或无监督学习问题，研究如何使得攻击者难以从拆分层表征中提取与标签关联的信息。
%
