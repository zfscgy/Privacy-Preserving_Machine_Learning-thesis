\cleardoublepage
\chapternonum{摘要}
随着机器学习的广泛应用，其隐私问题也越来越突出，因此许多\textbf{隐私保护机器学习方法}被提出，旨在保护数据隐私和模型隐私的同时，实现机器学习模型的高效推断和训练。
%
隐私保护机器学习中的两种重要方法为基于密码学的隐私计算和拆分学习。
基于密码学的方法拥有较高的安全性，但是其计算量和通信量较高，运行时间长，导致了实际应用中的效率低下问题。
拆分学习仅需交换模型计算的中间结果，无需复杂的加解密运算，因此效率相对密码学方法较高，但是相比于中心化训练依然存在较高的通信开销。
同时，在拆分学习过程中，部分模型以及模型中间结果的暴露也会带来多种隐私泄漏问题，导致其安全性受到质疑。
%

上述效率和安全问题对隐私保护机器学习的实际应用带来了重大的挑战。
%
因此，本文围绕着隐私保护机器学习中的效率和安全问题，进行了全面的分析研究，并且从拆分学习以及密码学和随机排列的混合方法两条路线对隐私保护机器学习中的效率和安全进行优化。
%
本文的主要贡献点包括：


\textbf{（1）针对拆分学习的通信量大问题，提出了基于随机Top-$k$稀疏化的通信压缩方法。}
尽管拆分学习相对于密码学方法效率较高，但是在拆分层表征尺寸大的情况下依然会带来一定的通信开销。
%
本文研究了稀疏、量化、拆分层缩小、L1正则化等经典的通信压缩手段，从理论上说明了在同等压缩率下稀疏化相比于缩小拆分层能够带来更大的表征空间从而提高模型泛化能力，同时展示了传统的Top-$k$稀疏化会带来部分神经元难以被训练的收敛性问题。
%
基于以上分析，本文提出了随机Top-$k$稀疏化方法，在提高训练和推断的通信效率的同时，尽可能减少了模型性能的损失。

\textbf{（2）针对拆分学习面临的模型补全攻击风险，提出了基于势能损失的隐私保护方法。}
拆分学习中，样本的拆分层表征和其类别标签往往有着较高的关联性，从而导致了模型补全攻击的隐患，即：攻击者可以利用少量泄漏的样本标签或者直接对拆分层表征上进行聚类，从而得到表征到标签的映射，窃取顶部模型以及样本标签。
%
为了防御模型补全攻击，本文将攻击过程看作攻击者的有监督或无监督学习过程，从泛化误差的角度说明当样本表征分布在决策边界时，攻击效果会降低。
%
受到电荷分布现象的启发，本文提出了基于势能损失函数的拆分学习隐私保护方法，通过在同类表征之间加入排斥力，使得样本表征分布在决策边界附近，相比于已有方法，显著降低了模型补全攻击的效果，同时实现最少的模型性能损失。
%

\textbf{（3）针对密码学方法非线性函数计算的性能瓶颈，提出了基于秘密分享和随机排列的高效隐私保护神经网络框架。}
密码学的隐私保护神经网络框架面临着非线性激活函数开销大的问题，而另一方面，拆分学习或其他混合方法存在着中间结果泄露隐私的问题。
%
本文基于随机排列协议，提出在半可信第三方上对随机排列的明文进行非线性逐元素函数的计算，并结合秘密分享，实现了高效的隐私保护神经网络推断与训练。%
%
此外，本文基于距离相关性指标对随机排列的安全性进行了详细的量化分析，表明了随机排列后的中间结果泄露信息可忽略不计。


\textbf{（4）针对大语言模型隐私推断难题，融合优化秘密分享、随机排列和同态加密，实现秒级别的隐私推断。}
大语言模型的参数量众多且计算量庞大，给隐私保护机器学习带来了新的挑战。
本文首先证明大模型中间结果几乎完全保留了输入信息，因此传统的拆分学习方法无法应用于大语言模型。
考虑到密码学方法在处理非线性函数时的高开销的问题，本文提出混合随机排列、秘密分享以及同态加密三种技术，针对大模型推断过程进行深度优化，建立高效的大模型隐私推断框架，并在开源大模型的基础上实现了秒级别的大模型隐私推断。

\noindent \textbf{关键词}: 隐私保护机器学习，拆分学习，纵向联邦学习


\cleardoublepage
\chapternonum{Abstract}
With the widespread application of machine learning, privacy concerns have become increasingly prominent.
In order to preserve the data privacy, model privacy, and computation efficiency during the inference and training phases of machine learning models, many privacy-preserving machine learning methods have been proposed.
%
Among these, two mainstream approaches are cryptographic methods and split learning.
%
Cryptographic methods achieve high security level, at the price of heavy computation and communication costs.
%
Split learning only requires the exchange of intermediate results, instead of cryptographic operations.
Thus, it is relatively more efficient compared with cryptographic methods, although still costing more communication than centralized training.
Moreover, the intermediate results and the partial models exposed in split learning could leak privacy, raising multiple security concerns.

The aforementioned efficiency and security issues present great challenge to the real-world applications of privacy-preserving machine learning.
%
This article aims at such efficiency and security problems, and conducts extensive analysis and research.
%
We propose optimizations from two aspects, i.e., split learning and combination of cryptographic methods and random permutation.
%
The main contributions of this article are listed below:


\textbf{(1) Randomized top-$k$ sparsification to reduce communication for split learning.}
Although split learning is more efficient than cryptographic methods, but it still has a certain level of communication cost when the split layer has a large size.
Currently, the studies on the communication efficiency are few.
%
In this article, we studied numerous classic compression methods, i.e, sparsification, quantization, reducing the cut layer size, and L1 regularization.
%
We demonstrate that top-$k$ sparsification obtains a larger feature space than reducing the cut layer size, leading to a better generalization, while classic top-$k$ sparsification also has the convergence problem due to the dead neuron problem.
%
Based on the above analysis, we propose randomized top-$k$ sparsification, which improves the training and inference efficiency of split learning, and minimizes the model performance loss.
%

\textbf{(2) Potential energy loss to protect split learning against model completion attack.}
In split learning, the forward embeddings are highly correlated with the sample labels, resulting in the vulnerability against model completion attacks.
%
The attackers can either use a few leaked labeled samples, or cluster on the forward embeddings, to learn a mapping from the embedding to the label, and steak the top model and sample labels.
%
To defend model completion attacks, we first view the attack process as a supervised or unsupervised learning process, and demonstrate that distributing embeddings near the decision boundaries reduces attack performance by analysis of the generalization error.
%
Inspired by the phenomenon of electrostatic equilibrium , we propose the potential energy loss to protect split learning, by adding repulsive forces between same-class sample pairs.
%
Compared with existing measures, potential energy loss significantly reduces the performance of the model completion attack, and minimizes the accuracy loss of the original task.


\textbf{(3) Accelerating the nonlinear function evaluation in privacy-preserving neural network based on combining secret sharing and random permutation.}
Cryptographic privacy-preserving neural network frameworks exhibit heavy overheads on nonlinear activation functions.
On the other side, split learning and other hybrid methods have privacy concerns since they directly reveal the intermediate results.
%
This article proposes the privacy-preserving nonlinear element-wise function evaluation based on random permutation and a semi-honest third-party.
%
Combining it with secret sharing, we realize efficient privacy-preserving neural network training and inference.
%
Moreover, we conduct extensive quantitative analysis on the privacy leakage of random permutation based on the distance correlation metric, demonstrating the privacy leakage is negligible.

\textbf{(4) Achieving secure large language model inference within seconds based on improved secret sharing, random permutation, and homomorphic encryption.}
Large language models contain a large amount of parameters and require heavy computation, which presents new challenges to privacy preserving machine learning.
%
In this article, we first show that the intermediate embeddings in large language models almost fully preserve the input text, so classic split learning approach is not applicable.
%
Then we focus on the nonlinear evaluation bottleneck in cryptographic methods, and propose an efficient large language model private inference framework based on secret sharing, random permutation, and homomorphic encryption.
%
We also implement a open-source large language model based on our framework and realize private inference within seconds.
%

\noindent \textbf{keywords}: Privacy-Preserving Machine Learning, Split Learning, Vertical Federated Learning